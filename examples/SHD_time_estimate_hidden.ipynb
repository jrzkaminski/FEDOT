{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "parentdir = '/home/jerzy/Documents/GitHub/jrzkaminski/FEDOT/'\n",
    "bamtdir = '/home/jerzy/Documents/GitHub/jrzkaminski/BAMT'\n",
    "sys.path.insert(0, parentdir)\n",
    "sys.path.insert(0, bamtdir)\n",
    "\n",
    "from math import ceil\n",
    "from pgmpy.estimators import K2Score\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from fedot.core.pipelines.convert import graph_structure_as_nx_graph\n",
    "from fedot.core.optimisers.optimizer import GraphGenerationParams\n",
    "from fedot.core.optimisers.graph import OptGraph, OptNode\n",
    "from fedot.core.optimisers.objective.objective_eval import ObjectiveEvaluate\n",
    "from fedot.core.optimisers.objective.objective import Objective\n",
    "from fedot.core.optimisers.gp_comp.operators.selection import SelectionTypesEnum\n",
    "from fedot.core.optimisers.gp_comp.gp_optimiser import EvoGraphOptimiser, GPGraphOptimiserParameters, \\\n",
    "    GeneticSchemeTypesEnum\n",
    "from fedot.core.optimisers.adapters import DirectAdapter\n",
    "from fedot.core.dag.verification_rules import has_no_cycle, has_no_self_cycled_nodes\n",
    "from examples.divided_bn import DividedBN\n",
    "from fedot.core.composer.gp_composer.gp_composer import PipelineComposerRequirements\n",
    "import bamt.preprocessors as pp\n",
    "import bamt.networks as Nets\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from fedot.core.dag.graph import Graph\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pigs = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/pigs.csv')\n",
    "win95pts = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/win95pts.csv')\n",
    "hailfinder = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/hailfinder.csv')\n",
    "hepar2 = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/hepar2.csv')\n",
    "arth150 = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/arth150.csv')\n",
    "ecoli70 = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/ecoli70.csv', index_col=0)\n",
    "magic_irri = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/magic-irri.csv', index_col=0)\n",
    "magic_niab = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/magic-niab.csv', index_col=0)\n",
    "\n",
    "pigs_true = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/pigs_true.csv')\n",
    "win95pts_true = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/win95pts_true.csv')\n",
    "hailfinder_true = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/hailfinder_true.csv')\n",
    "hepar2_true = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/hepar2_true.csv')\n",
    "arth150_true = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/arth150_true.csv')\n",
    "ecoli70_true = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/ecoli70_true.csv')\n",
    "magic_irri_true = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/magic-irri_true.csv')\n",
    "magic_niab_true = pd.read_csv('https://raw.githubusercontent.com/jrzkaminski/BAMT-old/main/data/magic-niab_true.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Featured functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomGraphModel(Graph):\n",
    "\n",
    "    def __init__(self, nodes: Optional[Union[OptNode, List[OptNode]]] = None):\n",
    "        super().__init__(nodes)\n",
    "        self.unique_pipeline_id = 1\n",
    "\n",
    "\n",
    "class CustomGraphNode(OptNode):\n",
    "    def __str__(self):\n",
    "        return self.content[\"name\"]\n",
    "\n",
    "\n",
    "def child_dict(net: list):\n",
    "    res_dict = dict()\n",
    "    for e0, e1 in net:\n",
    "        if e1 in res_dict:\n",
    "            res_dict[e1].append(e0)\n",
    "        else:\n",
    "            res_dict[e1] = [e0]\n",
    "    return res_dict\n",
    "\n",
    "def precision_recall(pred_net: list, true_net: list, decimal = 2):\n",
    "    pred_dict = child_dict(pred_net)\n",
    "    true_dict = child_dict(true_net)\n",
    "    corr_undir = 0\n",
    "    corr_dir = 0\n",
    "    for e0, e1 in pred_net:\n",
    "        flag = True\n",
    "        if e1 in true_dict:\n",
    "            if e0 in true_dict[e1]:\n",
    "                corr_undir += 1\n",
    "                corr_dir += 1\n",
    "                flag = False\n",
    "        if (e0 in true_dict) and flag:\n",
    "            if e1 in true_dict[e0]:\n",
    "                corr_undir += 1\n",
    "    pred_len = len(pred_net)\n",
    "    true_len = len(true_net)\n",
    "    shd = pred_len + true_len - corr_undir - corr_dir\n",
    "    return {'AP': round(corr_undir/pred_len, decimal), 'AR': round(corr_undir/true_len, decimal), 'AHP': round(corr_dir/pred_len, decimal), 'AHR': round(corr_dir/true_len, decimal), 'SHD': shd}\n",
    "\n",
    "\n",
    "# задаем метрику\n",
    "def custom_metric(graph: CustomGraphModel, data: pd.DataFrame):\n",
    "    graph_nx, labels = graph_structure_as_nx_graph(graph)\n",
    "    struct = []\n",
    "    for meta_edge in graph_nx.edges():\n",
    "        l1 = str(labels[meta_edge[0]])\n",
    "        l2 = str(labels[meta_edge[1]])\n",
    "        struct.append([l1, l2])\n",
    "\n",
    "    bn_model = BayesianNetwork(struct)\n",
    "    bn_model.add_nodes_from(data.columns)\n",
    "\n",
    "    score = K2Score(data).score(bn_model)\n",
    "    return [-score]\n",
    "\n",
    "\n",
    "# задаем кроссовер (обмен ребрами)\n",
    "def custom_crossover_exchange_edges(graph_first: OptGraph, graph_second: OptGraph, max_depth):\n",
    "    def find_node(graph: OptGraph, node):\n",
    "        return graph.nodes[dir_of_nodes[node.content['name']]]\n",
    "\n",
    "    num_cros = 100\n",
    "    try:\n",
    "        for _ in range(num_cros):\n",
    "            new_graph_first = deepcopy(graph_first)\n",
    "            new_graph_second = deepcopy(graph_second)\n",
    "\n",
    "            edges_1 = new_graph_first.operator.get_all_edges()\n",
    "            edges_2 = new_graph_second.operator.get_all_edges()\n",
    "            count = ceil(min(len(edges_1), len(edges_2)) / 2)\n",
    "            choice_edges_1 = random.sample(edges_1, count)\n",
    "            choice_edges_2 = random.sample(edges_2, count)\n",
    "\n",
    "            for meta_edge in choice_edges_1:\n",
    "                new_graph_first.operator.disconnect_nodes(meta_edge[0], meta_edge[1], False)\n",
    "            for meta_edge in choice_edges_2:\n",
    "                new_graph_second.operator.disconnect_nodes(meta_edge[0], meta_edge[1], False)\n",
    "\n",
    "            old_edges1 = new_graph_first.operator.get_all_edges()\n",
    "            old_edges2 = new_graph_second.operator.get_all_edges()\n",
    "\n",
    "            new_edges_2 = [[find_node(new_graph_second, i[0]), find_node(new_graph_second, i[1])]\n",
    "                           for i in choice_edges_1]\n",
    "            new_edges_1 = [[find_node(new_graph_first, i[0]), find_node(new_graph_first, i[1])] for i in choice_edges_2]\n",
    "            for meta_edge in new_edges_1:\n",
    "                if meta_edge not in old_edges1:\n",
    "                    new_graph_first.operator.connect_nodes(meta_edge[0], meta_edge[1])\n",
    "            for meta_edge in new_edges_2:\n",
    "                if meta_edge not in old_edges2:\n",
    "                    new_graph_second.operator.connect_nodes(meta_edge[0], meta_edge[1])\n",
    "\n",
    "            if old_edges1 != new_graph_first.operator.get_all_edges() or old_edges2 != new_graph_second.operator.get_all_edges():\n",
    "                break\n",
    "\n",
    "        if old_edges1 == new_graph_first.operator.get_all_edges() and new_edges_1 != [] and new_edges_1 != None:\n",
    "            new_graph_first = deepcopy(graph_first)\n",
    "        if old_edges2 == new_graph_second.operator.get_all_edges() and new_edges_2 != [] and new_edges_2 != None:\n",
    "            new_graph_second = deepcopy(graph_second)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    return new_graph_first, new_graph_second\n",
    "\n",
    "\n",
    "# задаем три варианта мутации: добавление узла, удаление узла, разворот узла\n",
    "def custom_mutation_add(graph: OptGraph, **kwargs):\n",
    "    num_mut = 100\n",
    "    try:\n",
    "        for _ in range(num_mut):\n",
    "            rid = random.choice(range(len(graph.nodes)))\n",
    "            random_node = graph.nodes[rid]\n",
    "            other_random_node = graph.nodes[random.choice(range(len(graph.nodes)))]\n",
    "            nodes_not_cycling = (random_node.descriptive_id not in\n",
    "                                 [n.descriptive_id for n in other_random_node.ordered_subnodes_hierarchy()] and\n",
    "                                 other_random_node.descriptive_id not in\n",
    "                                 [n.descriptive_id for n in random_node.ordered_subnodes_hierarchy()])\n",
    "            if nodes_not_cycling:\n",
    "                graph.operator.connect_nodes(random_node, other_random_node)\n",
    "                break\n",
    "\n",
    "    except Exception as ex:\n",
    "        graph.log.warn(f'Incorrect connection: {ex}')\n",
    "    return graph\n",
    "\n",
    "\n",
    "def custom_mutation_delete(graph: OptGraph, **kwargs):\n",
    "    num_mut = 100\n",
    "    try:\n",
    "        for _ in range(num_mut):\n",
    "            rid = random.choice(range(len(graph.nodes)))\n",
    "            random_node = graph.nodes[rid]\n",
    "            other_random_node = graph.nodes[random.choice(range(len(graph.nodes)))]\n",
    "            if random_node.nodes_from is not None and other_random_node in random_node.nodes_from:\n",
    "                graph.operator.disconnect_nodes(other_random_node, random_node, False)\n",
    "                break\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def custom_mutation_reverse(graph: OptGraph, **kwargs):\n",
    "    num_mut = 100\n",
    "    try:\n",
    "        for _ in range(num_mut):\n",
    "            rid = random.choice(range(len(graph.nodes)))\n",
    "            random_node = graph.nodes[rid]\n",
    "            other_random_node = graph.nodes[random.choice(range(len(graph.nodes)))]\n",
    "            if random_node.nodes_from is not None and other_random_node in random_node.nodes_from:\n",
    "                graph.operator.reverse_edge(other_random_node, random_node)\n",
    "                break\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    return graph\n",
    "\n",
    "\n",
    "# задаем правила на запрет дублирующих узлов\n",
    "def _has_no_duplicates(graph):\n",
    "    _, labels = graph_structure_as_nx_graph(graph)\n",
    "    if len(labels.values()) != len(set(labels.values())):\n",
    "        raise ValueError('Custom graph has duplicates')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_edges_by_localstructures(data, datatype=\"discrete\", max_local_structures=8):\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    global local_edges, root_nodes, child_nodes, initial_df\n",
    "\n",
    "    initial_df = data\n",
    "\n",
    "    # initialize divided_bn\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    divided_bn = DividedBN(data=data, max_local_structures=max_local_structures)\n",
    "\n",
    "    divided_bn.set_local_structures(data, datatype=datatype)\n",
    "\n",
    "    local_edges = divided_bn.local_structures_edges\n",
    "\n",
    "    divided_bn.set_hidden_nodes(data=data)\n",
    "\n",
    "    hidden_df = pd.DataFrame.from_dict(divided_bn.hidden_nodes)\n",
    "\n",
    "    hidden_df.columns = hidden_df.columns.astype(str)\n",
    "\n",
    "    root_nodes = divided_bn.root_nodes\n",
    "    child_nodes = divided_bn.child_nodes\n",
    "\n",
    "    vertices = list(hidden_df.columns)\n",
    "\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    discretizer = preprocessing.KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "    p = pp.Preprocessor([('encoder', encoder), ('discretizer', discretizer)])\n",
    "    discretized_data, _ = p.apply(hidden_df)\n",
    "\n",
    "    # словарь: {имя_узла: уникальный_номер_узла}\n",
    "    global dir_of_nodes\n",
    "    dir_of_nodes = {hidden_df.columns[i]: i for i in range(len(hidden_df.columns))}\n",
    "\n",
    "    # правила для байесовских сетей: нет петель, нет циклов, нет повторяющихся узлов\n",
    "    rules = [has_no_self_cycled_nodes, has_no_cycle, _has_no_duplicates]\n",
    "\n",
    "    # задаем для оптимизатора fitness-функцию\n",
    "    objective = Objective(custom_metric)\n",
    "    objective_eval = ObjectiveEvaluate(objective, data=discretized_data)\n",
    "    # инициализация начальной сети (пустая)\n",
    "    initial = [CustomGraphModel(nodes=[CustomGraphNode(nodes_from=None,\n",
    "                                                       content={'name': vertex}) for vertex in vertices])]\n",
    "\n",
    "    requirements = PipelineComposerRequirements(\n",
    "        primary=vertices,\n",
    "        secondary=vertices,\n",
    "        max_arity=100,\n",
    "        max_depth=100,\n",
    "        pop_size=15,\n",
    "        num_of_generations=50,\n",
    "        crossover_prob=0.8,\n",
    "        mutation_prob=0.9\n",
    "    )\n",
    "\n",
    "    optimiser_parameters = GPGraphOptimiserParameters(\n",
    "        genetic_scheme_type=GeneticSchemeTypesEnum.steady_state,\n",
    "        selection_types=[SelectionTypesEnum.tournament],\n",
    "        mutation_types=[custom_mutation_add, custom_mutation_delete, custom_mutation_reverse],\n",
    "        crossover_types=[custom_crossover_exchange_edges]\n",
    "    )\n",
    "\n",
    "    graph_generation_params = GraphGenerationParams(\n",
    "        adapter=DirectAdapter(base_graph_class=CustomGraphModel, base_node_class=CustomGraphNode),\n",
    "        rules_for_constraint=rules)\n",
    "\n",
    "    optimiser = EvoGraphOptimiser(\n",
    "        graph_generation_params=graph_generation_params,\n",
    "        parameters=optimiser_parameters,\n",
    "        requirements=requirements,\n",
    "        initial_graph=initial,\n",
    "        objective=objective)\n",
    "\n",
    "    # запуск оптимизатора\n",
    "    optimized_graph = optimiser.optimise(objective_eval)[0]\n",
    "    # вывод полученного графа\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    evolutionary_edges = optimized_graph.operator.get_all_edges()\n",
    "\n",
    "    # print('evolutionary_edges', evolutionary_edges)\n",
    "\n",
    "    # optimized_graph.show()\n",
    "\n",
    "    local_edges_merged = []\n",
    "\n",
    "    for key in local_edges:\n",
    "         local_edges_merged += local_edges[key]\n",
    "\n",
    "    # print('local_edges_merged', local_edges_merged)\n",
    "\n",
    "    external_edges = []\n",
    "\n",
    "    for meta_edge in evolutionary_edges:\n",
    "        for root_node in root_nodes[int(str(meta_edge[0]))]:\n",
    "            for child_node in child_nodes[int(str(meta_edge[1]))]:\n",
    "                external_edges.append([root_node, child_node])\n",
    "\n",
    "\n",
    "    # unpacked_hidden_edges = []\n",
    "    # unpacked_hidden_edges = np.array(unpacked_hidden_edges)\n",
    "\n",
    "    # for meta_edge in evolutionary_edges:\n",
    "    #     local_root_nodes = root_nodes[int(str(meta_edge[0]))]\n",
    "    #     local_child_nodes = child_nodes[int(str(meta_edge[1]))]\n",
    "    #     for root_node in local_root_nodes:\n",
    "    #         for child_node in local_child_nodes:\n",
    "    #             unpacked_hidden_edges = np.append(unpacked_hidden_edges, [[root_node, child_node]], axis=0)\n",
    "\n",
    "    # print('Unpacked hidden edges:', unpacked_hidden_edges)\n",
    "\n",
    "    all_edges = local_edges_merged + external_edges\n",
    "\n",
    "    return all_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(custom_dag_edges, reference_dag_edges):\n",
    "    # Calculate True Positives (tp), False Positives (fp), and False Negatives (fn)\n",
    "    tp = fp = fn = 0\n",
    "    for edge in custom_dag_edges:\n",
    "        if edge in reference_dag_edges:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    for edge in reference_dag_edges:\n",
    "        if edge not in custom_dag_edges:\n",
    "            fn += 1\n",
    "\n",
    "    # Calculate Precision and Recall\n",
    "    if tp + fp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def convert_to_string(nested_list):\n",
    "    return [[str(item) for item in inner_list] for inner_list in nested_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pigs_evo = get_edges_by_localstructures(pigs, datatype='discrete', max_local_structures=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision_recall(convert_to_string(pigs_evo), pigs_true.values.tolist())['SHD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(convert_to_string(pigs_evo), pigs_true.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win95pts_evo = get_edges_by_localstructures(win95pts, max_local_structures=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(win95pts_true.values.tolist(), convert_to_string(win95pts_evo))['SHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(win95pts_true.values.tolist(), convert_to_string(win95pts_evo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hailfinder_evo = get_edges_by_localstructures(hailfinder, max_local_structures=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(hailfinder_true.values.tolist(), convert_to_string(hailfinder_evo))['SHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(hailfinder_true.values.tolist(), convert_to_string(hailfinder_evo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hepar2_evo = get_edges_by_localstructures(hepar2, max_local_structures=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(hepar2_true.values.tolist(), hepar2_evo)['SHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(hepar2_true.values.tolist(), hepar2_evo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arth150_evo = get_edges_by_localstructures(arth150, datatype='continuous', max_local_structures=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(arth150_true.values.tolist(), arth150_evo)['SHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli70_evo = get_edges_by_localstructures(ecoli70, datatype='continuous', max_local_structures=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(ecoli70_true.values.tolist(), ecoli70_evo)['SHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_irri_evo = get_edges_by_localstructures(magic_irri, datatype='continuous', max_local_structures=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(magic_irri_true.values.tolist(), magic_irri_evo)['SHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_niab_evo = get_edges_by_localstructures(magic_niab, datatype='continuous', max_local_structures=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(magic_niab_true.values.tolist(), magic_niab_evo)['SHD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = pd.read_csv('/Users/jerzypro/Documents/GitHub/FEDOT/examples/pres.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = stat.iloc[[5,6,8,9,10,11,12,13]]\n",
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "stat.plot(x=\"dataset\", y=[\"shd_bidag\", \"shd_brave\", \"shd_ga_hidden\"], kind=\"bar\",figsize=(20,10))\n",
    "plt.xlabel('Dataset', fontdict={'fontsize': 22})\n",
    "plt.ylabel('SHD', fontdict={'fontsize': 22})\n",
    "plt.title('Structural hamming distance for different benchmark datasets', fontdict={'fontsize': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "stat.plot(x=\"dataset\", y=[\"time_bidag\", \"time_brave\", \"time_ga_hidden\"], kind=\"bar\",figsize=(20,10))\n",
    "plt.xlabel('Dataset', fontdict={'fontsize': 22})\n",
    "plt.ylabel('time, s', fontdict={'fontsize': 22})\n",
    "plt.title('Time for different benchmark datasets', fontdict={'fontsize': 20})\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pigs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# define the range of variable counts you want to test\n",
    "variable_counts = range(100, 200, 20)\n",
    "\n",
    "# create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# loop through the variable counts and time the execution of your function\n",
    "for count in variable_counts:\n",
    "    # select the first `count` columns from your DataFrame\n",
    "    data = df.iloc[:, :count]\n",
    "    # calculate the maximum local structures based on the selected number of variables\n",
    "    max_local_structures = count // 25\n",
    "    # run the function 5 times and record the durations\n",
    "    durations = []\n",
    "    for i in range(3):\n",
    "        start_time = time.time()\n",
    "        get_edges_by_localstructures(data, datatype='discrete', max_local_structures=max_local_structures)\n",
    "        duration = time.time() - start_time\n",
    "        durations.append(duration)\n",
    "    # take the average duration of the 5 runs and store the result in a dictionary\n",
    "    result = {'variables': count, 'duration': sum(durations) / len(durations)}\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reults_amd_5800x3d = [{'variables': 100, 'duration': 39.65796987215678},\n",
    " {'variables': 110, 'duration': 49.2115273475647},\n",
    " {'variables': 120, 'duration': 57.53783130645752},\n",
    " {'variables': 130, 'duration': 61.85720213254293},\n",
    " {'variables': 140, 'duration': 59.63492830594381},\n",
    " {'variables': 150, 'duration': 77.87671264012654},\n",
    " {'variables': 160, 'duration': 87.72854121526082},\n",
    " {'variables': 170, 'duration': 87.38114698727925},\n",
    " {'variables': 180, 'duration': 90.07503477732341},\n",
    " {'variables': 190, 'duration': 97.80088567733765},\n",
    " {'variables': 200, 'duration': 92.55843989054362},\n",
    " {'variables': 210, 'duration': 112.06234796841939},\n",
    " {'variables': 220, 'duration': 119.9249399503072},\n",
    " {'variables': 230, 'duration': 133.53242739041647},\n",
    " {'variables': 240, 'duration': 129.52960761388144},\n",
    " {'variables': 250, 'duration': 133.55799412727356},\n",
    " {'variables': 260, 'duration': 146.34153779347739},\n",
    " {'variables': 270, 'duration': 166.4611929257711},\n",
    " {'variables': 280, 'duration': 165.3845240275065},\n",
    " {'variables': 290, 'duration': 143.90822863578796},\n",
    " {'variables': 300, 'duration': 164.4334537188212},\n",
    " {'variables': 310, 'duration': 208.56076248486838},\n",
    " {'variables': 320, 'duration': 198.76508418718973},\n",
    " {'variables': 330, 'duration': 204.2042313416799},\n",
    " {'variables': 340, 'duration': 196.72765175501505},\n",
    " {'variables': 350, 'duration': 208.47475425402322},\n",
    " {'variables': 360, 'duration': 209.39110573132834},\n",
    " {'variables': 370, 'duration': 245.15379524230957},\n",
    " {'variables': 380, 'duration': 249.4655213356018},\n",
    " {'variables': 390, 'duration': 253.62290811538696},\n",
    " {'variables': 400, 'duration': 274.08744796117145},\n",
    " {'variables': 410, 'duration': 214.6125543912252},\n",
    " {'variables': 420, 'duration': 236.08180085817972},\n",
    " {'variables': 430, 'duration': 241.00591564178467},\n",
    " {'variables': 440, 'duration': 241.12759121259054}]\n",
    " \n",
    "\n",
    "results_m1_pro_8core = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# create lists of x and y values for the plot\n",
    "x_values = [result['variables'] for result in reults_amd_5800x3d]\n",
    "y_values = [result['duration'] for result in reults_amd_5800x3d]\n",
    "\n",
    "# fit a polynomial curve to the data\n",
    "poly_degree = 2  # degree of the polynomial regression\n",
    "poly_features = PolynomialFeatures(degree=poly_degree)\n",
    "x_poly = poly_features.fit_transform(np.array(x_values).reshape(-1, 1))\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(x_poly, y_values)\n",
    "poly_y_pred = poly_model.predict(x_poly)\n",
    "\n",
    "# create the plot\n",
    "plt.scatter(x_values, y_values)\n",
    "plt.plot(x_values, poly_y_pred, color='red')\n",
    "plt.title('Execution Time vs. Number of Variables, Ryzen 5800x3d')\n",
    "plt.xlabel('Number of Variables')\n",
    "plt.ylabel('Execution Time (s)')\n",
    "plt.show()\n",
    "\n",
    "# estimate the time complexity of the algorithm\n",
    "coef = poly_model.coef_\n",
    "complexity_str = 'O('\n",
    "for i in range(len(coef)):\n",
    "    if i == 0:\n",
    "        complexity_str += str(round(coef[i], 2))\n",
    "    else:\n",
    "        complexity_str += f'+{str(round(coef[i], 2))}*n^{str(i)}'\n",
    "complexity_str += ')'\n",
    "print(f\"Estimated time complexity: {complexity_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsevo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dbeb5dcf851b2f7c45d83bc5b9b92c5e7fba8de5ef2ca4003b978e17cdaa303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
